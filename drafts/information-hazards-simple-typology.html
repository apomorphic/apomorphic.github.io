<!-- Single article page with comments -->
<!DOCTYPE html>
<html lang="en">
    <head>

        <meta charset="utf-8">
        <title>Information hazards: a very simple typology - Apomorphic</title>
        <meta name="description" content="">
        <meta name="author" content="Will Bradshaw">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
        <!--[if lt IE 9]>
            <script src="https://apomorphic.com/theme/html5.js"></script>
        <![endif]-->


        <!-- Le styles -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
        <link href="https://apomorphic.com/theme/local.css" rel="stylesheet">
        <link href="https://apomorphic.com/theme/pygments.css" rel="stylesheet">
        <link href="https://apomorphic.com/theme/font-awesome.css" rel="stylesheet">
        <link href='https://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>

    </head>

    <body>
        <header class="blog-header">
            <div class="container">
                <div class="row-fluid">
                    <div class="col-sm-8">
                        <div class="container">
                            <!-- Website headline -->
                            <div class="row-fluid">
                                <a href="https://apomorphic.com" class="brand">Apomorphic</a>
                            </div>
                            <!-- Website tagline/subtitle -->
                            <div class="row-fluid">
                                <span class="tagline">A blog about biology, self-improvement, and the future</span>
                            </div>
                        </div>
                    </div>

                    <!-- Pages in links on right -->
                    <div class="col-sm-4" id="blog-nav">
                        <ul class="nav nav-pills pull-right">
                            <li><a href="https://apomorphic.com/about">About</a></li>
                        </ul>
                    </div>
                </div> <!-- End of fluid row-->
            </div>   <!-- End of Container-->
        </header>

        <main>
        <! -- Main content of page, depending on page type -->
        <div class="container">
            <div class="content">
                <div class="row-fluid">
                    <div class="col-sm-12">
<!-- Main article content (as on homepage) -->
<div class="container">
<!-- Post title -->
<div class="row-fluid">
    <div class="article-title col-md-10 col-sm-12 col-xs-12 col-md-offset-1">
        <a href="https://apomorphic.com/drafts/information-hazards-simple-typology" id="title">Information hazards: a very simple&nbsp;typology</a>
    </div>
</div>

<!-- Post info -->
<div class="row-fluid">
    <div class="article-info col-md-10 col-sm-12 col-xs-12 col-md-offset-1">
        <p>Posted on Wed 24 June 2020 by Will Bradshaw</p>
    </div>
</div>

<!-- Post content -->
<div class="row-fluid">
    <div class="article-content col-md-10 col-sm-12 col-xs-12 col-md-offset-1">
        <p>It seems that everyone who looks into information hazards eventually develops their own typology. Perhaps this isn&#8217;t so surprising; <a href="https://www.nickbostrom.com/information-hazards.pdf">Bostrom&#8217;s original paper</a> lists about thirty different categories of information hazards and isn&#8217;t even completely exhaustive, and those who wish to work with a simpler system have many ways of dividing up that&nbsp;space.</p>
<p>In general, I&#8217;ve tended to use a typology adapted from Anders Sandberg, with minor tweaks to fit it better into my own brain.This typology divides the majority of infohazards into three broad types<sup id="fnref:true"><a class="footnote-ref" href="#fn:true">1</a></sup>:</p>
<ol>
<li>
<p><strong>Capability hazards:</strong> Information that gives other actors new or improved abilities to harm you or your values<sup id="fnref:capability"><a class="footnote-ref" href="#fn:capability">2</a></sup>, either by acting in ways you wish they would not, or by threatening to do so to extort actions from you.
<em>Examples:</em> Instructions for building nuclear weapons; incriminating information suitable for blackmail; most biosecurity infohazards; that <a href="https://www.schneier.com/essays/archives/2003/03/locks_and_full_discl.html">master key</a>&nbsp;hack.</p>
</li>
<li>
<p><strong>Direct hazards<sup id="fnref:memetic"><a class="footnote-ref" href="#fn:memetic">3</a></sup>:</strong> Information that directly harms the possessor, either through infliction of negative emotional states<sup id="fnref:direct_other"><a class="footnote-ref" href="#fn:direct_other">4</a></sup> or by otherwise reducing their ability to achieve their goals.
<em>Examples:</em> News that a loved one has died unpleasantly; political news you can do nothing about but find very distracting; the examples from <a href="https://www.lesswrong.com/posts/Rut5wZ7qyHoj3dj4k/a-point-of-clarification-on-infohazard-terminology">this post</a>; a certain well-known example from the rationality&nbsp;community.</p>
</li>
<li>
<p><strong>Mindset hazards:</strong> Information that, while true, interacts with other actors&#8217; beliefs or biases in a way that causes them to act badly – either by acting rationally based on beliefs or values you hold to be false, or irrationally according to their own beliefs and values.
Alleged <em>examples</em> of this class of hazard are common but tend to be controversial<sup id="fnref:controversial"><a class="footnote-ref" href="#fn:controversial">5</a></sup>; some commonly cited examples include statistical differences in capabilities or character traits between demographic groups; the <a href="https://www.unaids.org/en/frequently-asked-questions-about-hiv-and-aids">generally low efficiency of <span class="caps">HIV</span> transmission</a>; </p>
</li>
</ol>
<p>Of course, these three categories all bleed into each other at the edges. The boundary between a direct hazard (that hurts the knower) and a mindset hazard (that hurts others via the actions of the knower) is especially fuzzy; information that makes someone act irrationally is <a href="https://www.lesswrong.com/posts/AdYdLP2sRqPMoe8fb/knowing-about-biases-can-hurt-people">likely to be both</a>. Some direct and mindset hazards are also capability hazards, in that an enemy or careless actor in possession of the information can hurt you by sharing it with you or your allies (or threatening to do so). But in general it is possible to tease apart these different kinds of harm, and identify the one that is of greatest concern in a given particular&nbsp;case.</p>
<hr>
<p>Having laid out my simple typology, we come to the more tendentious part of this post: I claim that concerns about direct and mindset hazards are <em>very rarely</em> sufficient justification for suppressing&nbsp;information.</p>
<p>This isn&#8217;t meant as an iron law of the universe: it&#8217;s easy enough to stipulate hypothetical cases where a direct or mindset hazard could cause serious harm, and weird hypotheticals come true from time to&nbsp;time. </p>
<p>And there are a few well-established real-world exceptions, like personal privacy, that I broadly support. But in virtually all other real-world cases, I think the world is better off with the information being publicly available in some form than with it hidden, and with strong norms and institutions&nbsp;that </p>
<p>Why do I think this? Firstly, I think there&#8217;s a general strong prior that it&#8217;s good for true information to be publicly available, and you need a strong special reason to override that. With capability hazards, one can sometimes make a credible case that the information being more widely known can do little benefit and much harm (or much benefit and <em>extremely severe</em> harm), but I think that&#8217;s quite rarely the case for direct and mindset&nbsp;hazards.</p>
<p>Direct hazards in particular seem quite rare in anything other than very mild form, and serious direct hazards are rarely serious for more than a few people, for ideosyncratic personal reasons. So if the information in question is of any public utility at all (and most information that doesn&#8217;t fall under &#8220;personal privacy&#8221; is) then the benefits of sharing it usually outweigh the&nbsp;costs.</p>
<p>For mindset hazards, the case is somewhat different. Before talking about dangerously <em>true</em>&nbsp;ideas</p>
<p>I think it&#8217;s fairly clear to most people that there are such things as dangerously wrong ideas that have done great net harm. The problem, of course, is twofold. Firstly, nobody can agree on which ideas those are, and history is littered with examples of people viciously suppressing ideas that turned out to be true and important. Secondly, the idea of suppressing dangerous ideas is itself one of the most blood-soaked in history, with vast amounts of death and suffering (not to mention tragically lost opportunities for intellectual progress) resulting from people&#8217;s&nbsp;belief</p>
<p>Suppression of falsehood has also proven an extremely tempting and effective excuse for those who instead – or additionally – wish to suppress&nbsp;dissent.</p>
<p>All that is about the suppression of ideas you believe to&nbsp;be </p>
<p>[I think I need to distinguish between &#8220;violently suppress&#8221;, &#8220;decline to spread&#8221;, &#8220;encourage others not to spread&#8221; and &#8220;forcibly encourage others not to&nbsp;spread&#8221;]</p>
<p>My reasoning is slightly different for direct vs mindset hazards. With direct hazards, I generally think they&#8217;re generally either pretty minor or very rare,&nbsp;and </p>
<ol>
<li>
<p>In almost all real-world cases,&nbsp;direct </p>
</li>
<li>
<p>As I said&nbsp;a</p>
</li>
<li>
<p>Mindset hazards in particular are extremely tempting targets for people who want to censor information for selfish or authoritarian reasons. Yes, <em>this particular</em> information isn&#8217;t heretical, but it 
might lead people into error
I think it&#8217;s fairly clear that unjustified suppression of information has so far done vastly more harm than unjustified spreading of information, and preventing mindset hazards has been a&nbsp;major </p>
</li>
</ol>
<div class="footnote">
<hr>
<ol>
<li id="fn:true">
<p>Note that, as with all infohazards, these categories only apply to <em>true</em> information. The harms of spreading false information are generally not controversial, or especially interesting.&#160;<a class="footnote-backref" href="#fnref:true" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:capability">
<p>Note that this definition is not limited to malicious actors. Information that provides careless or incompetent actors with new capacities to cause accidental harm also falls under this category.&#160;<a class="footnote-backref" href="#fnref:capability" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:memetic">
<p>Also known as memetic hazards or cognitohazards, though both of these sound more esoteric than ideal.&#160;<a class="footnote-backref" href="#fnref:memetic" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:direct_other">
<p>If there are mental states (&#8220;epistemic or attentional states&#8221;, as Bostrom puts it) other than positive/negative valence that are required for, or inimical to, wellbeing, then information that damages the good states or induces the bad ones would also count as a direct information hazard.&#160;<a class="footnote-backref" href="#fnref:direct_other" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:controversial">
<p>Implicit in the definition of mindset hazards is the assumption that the new information is true, and the actor&#8217;s prior beliefs (or values) are false. Naturally, the actor themselves will tend to strongly contest this claim, as will people who think the new information is false. 
If both the information and the prior beliefs are true, then the actor is simply acting rationally; if both are false, then you are either misleadingly playing off the weaknesses of others or recklessly exacerbating their prejudices. If the information is false and the prior beliefs are true, you&#8217;re just lying. All of these&#160;<a class="footnote-backref" href="#fnref:controversial" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div>
    </div>
</div>

<!-- Tags and categories -->
<div class="row-fluid">
    <div class="article-info col-md-10 col-sm-12 col-xs-12 col-md-offset-1">
        <p>Posted in <a href="https://apomorphic.com/category/drafts.html">drafts</a>  | Tagged as <a href="https://apomorphic.com/tag/information-hazards.html">information hazards</a>, <a href="https://apomorphic.com/tag/philosophy.html">philosophy</a>, <a href="https://apomorphic.com/tag/strategy.html">strategy</a>, <a href="https://apomorphic.com/tag/effective-altruism.html">effective altruism</a></p>
    </div>
</div></div>

<! -- Comments -->
<div class="container">
    <div class="article-comments col-sm-10">
        <h3 id="disqus_title_header">Comments</h3>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            var disqus_shortname = 'apomorphic';
            var disqus_title = 'Information hazards: a very simple&nbsp;typology';

(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
</div>

                    </div>
                </div>             </div>         </div>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>


    </body>
</html>